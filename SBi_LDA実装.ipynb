{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## SBi-LDA実装\n",
        "田村一樹さんらによる「評点付きレビュー文書を対象としたトピックモデルの構築に関する検討」で提案されたSBi-LDAを実装してみました。\n",
        "\n",
        "(https://ipsj.ixsq.nii.ac.jp/ej/?action=repository_action_common_download&item_id=141419&item_no=1&attribute_id=1&file_no=1)\n"
      ],
      "metadata": {
        "id": "31RllejvBfpH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW-jkaQCmcej"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from janome.tokenizer import Tokenizer\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "class SBiLDA:\n",
        "    def __init__(self, n_user_topics=10, n_item_topics=10, alpha_u=0.1, alpha_m=0.1, beta_u=0.1, beta_m=0.1, gamma=0.1):\n",
        "        # モデルのパラメータを初期化\n",
        "        self.n_user_topics = n_user_topics  # ユーザートピックの数\n",
        "        self.n_item_topics = n_item_topics  # アイテムトピックの数\n",
        "        self.alpha_u = alpha_u  # ユーザートピックのDirichlet事前分布のパラメータ\n",
        "        self.alpha_m = alpha_m  # アイテムトピックのDirichlet事前分布のパラメータ\n",
        "        self.beta_u = beta_u  # ユーザートピック-単語分布のDirichlet事前分布のパラメータ\n",
        "        self.beta_m = beta_m  # アイテムトピック-単語分布のDirichlet事前分布のパラメータ\n",
        "        self.gamma = gamma  # スイッチング変数のBeta事前分布のパラメータ\n",
        "        self.tokenizer = Tokenizer(\"user_dictionary.csv\", udic_enc=\"utf8\")  # 日本語分かち書きのためのトークナイザー\n",
        "        self.stopwords = set(STOPWORDS)  # ストップワードのセット\n",
        "\n",
        "    def update_stopwords(self, new_stopwords):\n",
        "        # ストップワードを追加更新するメソッド\n",
        "        self.stopwords.update(new_stopwords)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # テキストを前処理するメソッド\n",
        "        if isinstance(text, str):\n",
        "            tokens = self.tokenizer.tokenize(text)\n",
        "            words = [token.surface for token in tokens\n",
        "                     if token.part_of_speech.split(',')[0] in ['名詞', '動詞', '形容詞']]\n",
        "            words = [word for word in words\n",
        "                     if word not in self.stopwords]\n",
        "            return words\n",
        "        return []\n",
        "\n",
        "    def process_texts(self, texts):\n",
        "        # 複数のテキストを処理するメソッド\n",
        "        results = []\n",
        "        for text in tqdm(texts, desc=\"Processing records\"):\n",
        "            results.append(self.preprocess_text(text))\n",
        "\n",
        "        all_words = [word for doc in results for word in doc]\n",
        "        word_counts = Counter(all_words)\n",
        "\n",
        "        filtered_results = []\n",
        "        for doc in results:\n",
        "            # 出現頻度が2以上の単語のみを保持\n",
        "            filtered_doc = [word for word in doc if word_counts[word] > 1]\n",
        "            filtered_results.append(filtered_doc)\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "    def prepare_corpus(self, df):\n",
        "        # コーパスを準備するメソッド\n",
        "        texts = self.process_texts(df[\"review_text\"])\n",
        "\n",
        "        vocabulary = set(word for doc in texts for word in doc)\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "\n",
        "        corpus = [[self.word_to_idx[word] for word in doc] for doc in texts]\n",
        "\n",
        "        self.n_users = df['user_id'].nunique()\n",
        "        self.n_items = df['item_id'].nunique()\n",
        "        self.n_vocab = len(self.word_to_idx)\n",
        "\n",
        "        return corpus, df['user_id'].tolist(), df['item_id'].tolist()\n",
        "\n",
        "    def fit(self, df, max_iter=100):\n",
        "        # モデルを学習するメソッド\n",
        "        corpus, users, items = self.prepare_corpus(df)\n",
        "\n",
        "        self.docs = corpus\n",
        "        self.users = users\n",
        "        self.items = items\n",
        "\n",
        "        # カウンタの初期化\n",
        "        self.nsv = np.zeros((self.n_user_topics, self.n_vocab))\n",
        "        self.ns = np.zeros(self.n_user_topics)\n",
        "        self.nus = np.zeros((self.n_users, self.n_user_topics))\n",
        "        self.nu = np.zeros(self.n_users)\n",
        "        self.nu0 = np.zeros(self.n_users)\n",
        "        self.nu1 = np.zeros(self.n_users)\n",
        "\n",
        "        self.ntv = np.zeros((self.n_item_topics, self.n_vocab))\n",
        "        self.nt = np.zeros(self.n_item_topics)\n",
        "        self.nmt = np.zeros((self.n_items, self.n_item_topics))\n",
        "        self.nm = np.zeros(self.n_items)\n",
        "\n",
        "        # トピックとスイッチ変数の初期化\n",
        "        self.z = []\n",
        "        self.y = []\n",
        "        for doc, user, item in zip(self.docs, self.users, self.items):\n",
        "            z_doc = []\n",
        "            y_doc = []\n",
        "            for word in doc:\n",
        "                y = np.random.randint(2)\n",
        "                if y == 0:\n",
        "                    z = np.random.randint(self.n_user_topics)\n",
        "                    self.nsv[z, word] += 1\n",
        "                    self.ns[z] += 1\n",
        "                    self.nus[user, z] += 1\n",
        "                    self.nu0[user] += 1\n",
        "                else:\n",
        "                    z = np.random.randint(self.n_item_topics)\n",
        "                    self.ntv[z, word] += 1\n",
        "                    self.nt[z] += 1\n",
        "                    self.nmt[item, z] += 1\n",
        "                    self.nu1[user] += 1\n",
        "                self.nu[user] += 1\n",
        "                self.nm[item] += 1\n",
        "                z_doc.append(z)\n",
        "                y_doc.append(y)\n",
        "            self.z.append(z_doc)\n",
        "            self.y.append(y_doc)\n",
        "\n",
        "        # ギブスサンプリング\n",
        "        for iteration in tqdm(range(max_iter), desc=\"Gibbs Sampling\"):\n",
        "            for i, (doc, user, item) in enumerate(zip(self.docs, self.users, self.items)):\n",
        "                for j, word in enumerate(doc):\n",
        "                    # 現在の割り当てを削除\n",
        "                    z = self.z[i][j]\n",
        "                    y = self.y[i][j]\n",
        "                    if y == 0:\n",
        "                        self.nsv[z, word] -= 1\n",
        "                        self.ns[z] -= 1\n",
        "                        self.nus[user, z] -= 1\n",
        "                        self.nu0[user] -= 1\n",
        "                    else:\n",
        "                        self.ntv[z, word] -= 1\n",
        "                        self.nt[z] -= 1\n",
        "                        self.nmt[item, z] -= 1\n",
        "                        self.nu1[user] -= 1\n",
        "                    self.nu[user] -= 1\n",
        "                    self.nm[item] -= 1\n",
        "\n",
        "                    # 新しい割り当てをサンプリング\n",
        "                    probs = np.zeros(self.n_user_topics + self.n_item_topics)\n",
        "                    for s in range(self.n_user_topics):\n",
        "                        probs[s] = ((self.nsv[s, word] + self.beta_u) / (self.ns[s] + self.n_vocab * self.beta_u)) * \\\n",
        "                                   ((self.nus[user, s] + self.alpha_u) / (self.nu[user] + self.n_user_topics * self.alpha_u)) * \\\n",
        "                                   ((self.nu0[user] + self.gamma) / (self.nu[user] + 2 * self.gamma))\n",
        "                    for t in range(self.n_item_topics):\n",
        "                        probs[self.n_user_topics + t] = ((self.ntv[t, word] + self.beta_m) / (self.nt[t] + self.n_vocab * self.beta_m)) * \\\n",
        "                                                        ((self.nmt[item, t] + self.alpha_m) / (self.nm[item] + self.n_item_topics * self.alpha_m)) * \\\n",
        "                                                        ((self.nu1[user] + self.gamma) / (self.nu[user] + 2 * self.gamma))\n",
        "\n",
        "                    # 正規化\n",
        "                    probs /= np.sum(probs)\n",
        "\n",
        "                    # 新しい割り当てを選択\n",
        "                    new_topic = np.random.choice(self.n_user_topics + self.n_item_topics, p=probs)\n",
        "                    if new_topic < self.n_user_topics:\n",
        "                        y = 0\n",
        "                        z = new_topic\n",
        "                        self.nsv[z, word] += 1\n",
        "                        self.ns[z] += 1\n",
        "                        self.nus[user, z] += 1\n",
        "                        self.nu0[user] += 1\n",
        "                    else:\n",
        "                        y = 1\n",
        "                        z = new_topic - self.n_user_topics\n",
        "                        self.ntv[z, word] += 1\n",
        "                        self.nt[z] += 1\n",
        "                        self.nmt[item, z] += 1\n",
        "                        self.nu1[user] += 1\n",
        "                    self.nu[user] += 1\n",
        "                    self.nm[item] += 1\n",
        "\n",
        "                    self.z[i][j] = z\n",
        "                    self.y[i][j] = y\n",
        "\n",
        "    def get_user_topic_distribution(self):\n",
        "        # ユーザーごとのトピック分布を取得\n",
        "        return (self.nus + self.alpha_u) / (self.nu[:, np.newaxis] + self.n_user_topics * self.alpha_u)\n",
        "\n",
        "    def get_item_topic_distribution(self):\n",
        "        # アイテムごとのトピック分布を取得\n",
        "        return (self.nmt + self.alpha_m) / (self.nm[:, np.newaxis] + self.n_item_topics * self.alpha_m)\n",
        "\n",
        "    def get_user_topic_word_distribution(self):\n",
        "        # ユーザートピックごとの単語分布を取得\n",
        "        return (self.nsv + self.beta_u) / (self.ns[:, np.newaxis] + self.n_vocab * self.beta_u)\n",
        "\n",
        "    def get_item_topic_word_distribution(self):\n",
        "        # アイテムトピックごとの単語分布を取得\n",
        "        return (self.ntv + self.beta_m) / (self.nt[:, np.newaxis] + self.n_vocab * self.beta_m)\n",
        "\n",
        "    def get_switch_probability(self):\n",
        "        # スイッチング確率を取得\n",
        "        return (self.nu0 + self.gamma) / (self.nu + 2 * self.gamma)\n",
        "\n",
        "    def visualize_topics(self, n_words=10, n_topics=None):\n",
        "        # トピックを可視化するメソッド\n",
        "        # 日本語フォントの設定\n",
        "        japanese_font = 'BIZ UDGothic'\n",
        "        plt.rcParams['font.family'] = japanese_font\n",
        "\n",
        "        user_topic_word_dist = self.get_user_topic_word_distribution()\n",
        "        item_topic_word_dist = self.get_item_topic_word_distribution()\n",
        "\n",
        "        if n_topics is None:\n",
        "            n_topics = max(self.n_user_topics, self.n_item_topics)\n",
        "\n",
        "        for topic_type, topic_word_dist in [(\"User\", user_topic_word_dist), (\"Item\", item_topic_word_dist)]:\n",
        "            for i in range(min(n_topics, len(topic_word_dist))):\n",
        "                topic_words = self.get_top_words(topic_word_dist[i], n_words)\n",
        "\n",
        "                print(f\"{topic_type} Topic {i}:\")\n",
        "                print(\", \".join(list(topic_words.keys())[:10]))  # 上位10単語を表示\n",
        "\n",
        "                self.generate_wordcloud(topic_words, i, f\"{topic_type} Topic\")\n",
        "                self.plot_top_words(topic_words, i, f\"{topic_type} Topic\", n_words)\n",
        "\n",
        "    def get_top_words(self, topic_word_dist, n=30):\n",
        "        # トピックの上位n個の単語を取得\n",
        "        topic_words = {}\n",
        "        for idx, prob in enumerate(topic_word_dist):\n",
        "            if idx in self.idx_to_word:\n",
        "                topic_words[self.idx_to_word[idx]] = prob\n",
        "        return dict(sorted(topic_words.items(), key=lambda x: x[1], reverse=True)[:n])\n",
        "\n",
        "    def generate_wordcloud(self, topic_words, topic_id, title):\n",
        "        # ワードクラウドを生成して表示\n",
        "        font_path = fm.findfont(fm.FontProperties(family='BIZ UDGothic'))\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
        "                              font_path=font_path, regexp=r\"[\\w']+\").generate_from_frequencies(topic_words)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'{title} {topic_id} - WordCloud')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_top_words(self, topic_words, topic_id, title, n_words=10):\n",
        "        # トピックの上位単語を棒グラフで表示\n",
        "        words = list(topic_words.keys())[:n_words]\n",
        "        weights = list(topic_words.values())[:n_words]\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.bar(range(n_words), weights, align='center')\n",
        "        plt.xticks(range(n_words), words, rotation=45, ha='right')\n",
        "        plt.title(f'{title} {topic_id} - Top {n_words} Words')\n",
        "        plt.xlabel('Words')\n",
        "        plt.ylabel('Weight')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# modelの学習\n",
        "用意するものは以下のカラムを含むDataFrame\n",
        "\n",
        "review_text：レビュー本文 形態素解析前の生のテキスト\n",
        "\n",
        "user_id：レビュアーID\n",
        "\n",
        "item_id：商品ID"
      ],
      "metadata": {
        "id": "i0pbGPhDhP1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# パラメータの説明\n",
        "n_user_topics：ユーザートピック数\n",
        "\n",
        "n_item_topics：アイテムトピック数\n",
        "\n",
        "alpha,beta,gamma：事前分布のパラメータ(ハイパーパラメータ)\n",
        "\n",
        "max_iter：ギブスサンプリングの最大反復回数"
      ],
      "metadata": {
        "id": "EdLbezgot4dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sbilda import SBiLDA\n",
        "\n",
        "# データを読み込む\n",
        "df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# モデルを初期化\n",
        "model = SBiLDA(n_user_topics=10, n_item_topics=10, alpha_u=0.1, alpha_m=0.1, beta_u=0.1, beta_m=0.1, gamma=0.1)\n",
        "\n",
        "# モデルを学習\n",
        "model.fit(df, max_iter=100)\n",
        "\n",
        "# 結果を可視化\n",
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "_VvaxVLim4HL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}